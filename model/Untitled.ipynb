{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d88c4ac-906d-409d-8aa3-266248b32254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Using cache found in C:\\Users\\duwjd/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-4-24 Python-3.11.7 torch-2.7.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "C:\\Users\\duwjd/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    11.883536\n",
      "Name: predicted_views, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duwjd\\AppData\\Local\\Temp\\ipykernel_820\\3745093418.py:497: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_df['predicted_views'] = y_pred\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch  # YOLOv5ëŠ” PyTorchë¡œ êµ¬í˜„ë¨\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "import webcolors\n",
    "import easyocr\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from konlpy.tag import Okt\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°\n",
    "title = 'ì € ë“œë””ì–´ ê°•ë‚¨ì— ìƒµ ì˜¤í”ˆí–ˆì–´ìš”ğŸ˜­ì·¨í–¥ ê°€ë“ ë‹´ì€ ì œ ìƒµì„ ì†Œê°œí•©ë‹ˆë‹¤ğŸ¤ | ì¸í…Œë¦¬ì–´ ë¸Œì´ë¡œê·¸ | ë°˜ì…€í”„ ì¸í…Œë¦¬ì–´ | ë·°í‹°ìƒµ ì¸í…Œë¦¬ì–´ | ì¬ìœ JEYU'\n",
    "thumbnail_url = 'https://i.ytimg.com/vi/IbxI43fHWnk/maxresdefault.jpg'\n",
    "duration = '08:30'\n",
    "subscriber_count = 200000\n",
    "\n",
    "# í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ pub_year, pub_month, pub_weekday êµ¬í•˜ê¸°\n",
    "current_time = datetime.now()\n",
    "pub_year = current_time.year\n",
    "pub_month = current_time.month\n",
    "pub_weekday = current_time.weekday()  # ì›”ìš”ì¼=0, ì¼ìš”ì¼=6\n",
    "\n",
    "# ì…ë ¥ë°›ì€ ë°ì´í„°ë¥¼ dictionaryë¡œ ì €ì¥\n",
    "data = {\n",
    "    \"title\": title,\n",
    "    \"thumbnail_url\": thumbnail_url,\n",
    "    \"duration\": duration,\n",
    "    \"subscriber_count\": subscriber_count,\n",
    "    \"pub_year\": pub_year,\n",
    "    \"pub_month\": pub_month,\n",
    "    \"pub_weekday\": pub_weekday\n",
    "}\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "df = pd.DataFrame([data])\n",
    "\n",
    "# OCR Reader ì„¤ì •\n",
    "ocr_reader = easyocr.Reader(['en', 'ko'])  # ì˜ì–´ & í•œê¸€ OCR ì§€ì›\n",
    "\n",
    "def convert_duration_to_minutes(duration_str):\n",
    "    try:\n",
    "        hours, minutes = map(int, duration_str.split(\":\"))\n",
    "        return hours * 60 + minutes\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting duration: {e}\")\n",
    "        return 0\n",
    "\n",
    "# 'duration' ì»¬ëŸ¼ì„ ë¶„ìœ¼ë¡œ ë³€í™˜\n",
    "df['duration'] = df['duration'].apply(convert_duration_to_minutes)\n",
    "\n",
    "# ì´ë¯¸ì§€ ë¡œë“œ (URLì—ì„œ ë‹¤ìš´ë¡œë“œ)\n",
    "def load_image_from_url(url):\n",
    "    fallback_urls = [\n",
    "        url,\n",
    "        url.replace('maxresdefault', 'sddefault'),\n",
    "        url.replace('sddefault', 'hqdefault'),\n",
    "        url.replace('hqdefault', 'mqdefault'),\n",
    "        url.replace('mqdefault', 'default')\n",
    "    ]\n",
    "\n",
    "    for new_url in fallback_urls:\n",
    "        try:\n",
    "            response = requests.get(new_url, stream=True, timeout=5)\n",
    "            response.raise_for_status()\n",
    "            image = np.asarray(bytearray(response.content), dtype=np.uint8)\n",
    "            image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "            if image is not None:\n",
    "                height, width, _ = image.shape\n",
    "                return image, height, width\n",
    "        except requests.RequestException:\n",
    "            print(f\"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {new_url}\")\n",
    "\n",
    "    raise ValueError(\"ëª¨ë“  URLì—ì„œ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ê·¸ë ˆì´ìŠ¤ì¼€ì¼ + ì´ì§„í™”)\n",
    "def preprocess_image(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    denoised_image = cv2.GaussianBlur(binary_image, (5, 5), 0)\n",
    "    return denoised_image\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„ì„ (OCR)\n",
    "def extract_text(image, confidence_threshold=0.7):\n",
    "    results = ocr_reader.readtext(image)\n",
    "    text_info = []\n",
    "    for (bbox, text, prob) in results:\n",
    "        if prob >= confidence_threshold:\n",
    "            (top_left, _, bottom_right, _) = bbox\n",
    "            x, y = int(top_left[0]), int(top_left[1])\n",
    "            width = int(bottom_right[0] - top_left[0])\n",
    "            height = int(bottom_right[1] - top_left[1])\n",
    "            area = width * height\n",
    "            \n",
    "            text_info.append({\n",
    "                \"text\": text,\n",
    "                \"x\": x, \"y\": y,\n",
    "                \"width\": width, \"height\": height,\n",
    "                \"area\": area,\n",
    "                \"probability\": prob\n",
    "            })\n",
    "    return text_info\n",
    "\n",
    "# YOLOv5 ëª¨ë¸ ë¡œë“œ (PyTorch Hub)\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # 'yolov5s'ëŠ” ì‘ì€ ëª¨ë¸ (ë¹ ë¦„)\n",
    "\n",
    "# ê°ì²´ íƒì§€ (YOLOv5)\n",
    "def detect_objects_with_yolov5(image):\n",
    "    if image is None:\n",
    "        return {\"objects\": [], \"central_focus\": False}\n",
    "    \n",
    "    # YOLOv5 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê°ì²´ ê°ì§€\n",
    "    results = model(image)  # ì´ë¯¸ì§€ì—ì„œ ê°ì²´ ê°ì§€\n",
    "    objects = []\n",
    "    central_focus = False\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # ê°ì§€ëœ ê°ì²´ë“¤\n",
    "    for det in results.xywh[0].cpu().numpy():  # ê²°ê³¼ëŠ” (x_center, y_center, width, height, confidence, class) í˜•ì‹\n",
    "        x_center, y_center, w, h, conf, cls = det\n",
    "        if conf > 0.5:  # confidence scoreê°€ 0.5 ì´ìƒì¸ ê°ì²´ë§Œ ì‚¬ìš©\n",
    "            label = results.names[int(cls)]  # ê°ì²´ì˜ í´ë˜ìŠ¤ ì´ë¦„ (ì˜ˆ: 'person', 'car', etc.)\n",
    "            x1 = int((x_center - w / 2) * width)\n",
    "            y1 = int((y_center - h / 2) * height)\n",
    "            x2 = int((x_center + w / 2) * width)\n",
    "            y2 = int((y_center + h / 2) * height)\n",
    "            objects.append({\"label\": label, \"x\": x1, \"y\": y1, \"width\": x2 - x1, \"height\": y2 - y1})\n",
    "            \n",
    "            # ì¤‘ì•™ì— ê°€ê¹Œìš´ì§€ í™•ì¸\n",
    "            if (width * 0.3) < x_center < (width * 0.7) and (height * 0.3) < y_center < (height * 0.7):\n",
    "                central_focus = True\n",
    "    \n",
    "    # ê°ì§€ëœ ê°ì²´ì™€ ì¤‘ì•™ì— ìˆëŠ”ì§€ ì—¬ë¶€ ë¦¬í„´\n",
    "    return {\"objects\": objects, \"central_focus\": central_focus}\n",
    "\n",
    "# ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ ë° ì´ë¦„ ë³€í™˜\n",
    "def closest_colour(requested_colour):\n",
    "    \"\"\"ì£¼ì–´ì§„ RGB ê°’ê³¼ ê°€ì¥ ê°€ê¹Œìš´ CSS3 ìƒ‰ìƒëª…ì„ ì°¾ìŒ\"\"\"\n",
    "    min_colours = {}\n",
    "    for name in webcolors.names(\"css3\"):\n",
    "        r_c, g_c, b_c = webcolors.name_to_rgb(name)\n",
    "        rd = (r_c - requested_colour[0]) ** 2\n",
    "        gd = (g_c - requested_colour[1]) ** 2\n",
    "        bd = (b_c - requested_colour[2]) ** 2\n",
    "        min_colours[(rd + gd + bd)] = name\n",
    "    return min_colours[min(min_colours.keys())]\n",
    "\n",
    "def get_color_name_from_rgb(r, g, b):\n",
    "    \"\"\"ì •í™•í•œ ìƒ‰ìƒì´ ìˆìœ¼ë©´ ë°˜í™˜, ì—†ìœ¼ë©´ ê°€ì¥ ê°€ê¹Œìš´ ìƒ‰ìƒëª… ë°˜í™˜\"\"\"\n",
    "    try:\n",
    "        return webcolors.rgb_to_name((r, g, b), spec='css3')\n",
    "    except ValueError:\n",
    "        return closest_colour((r, g, b))\n",
    "\n",
    "def extract_colors(image, num_colors=3):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).reshape((-1, 3))\n",
    "    kmeans = KMeans(n_clusters=num_colors, n_init=\"auto\")\n",
    "    labels = kmeans.fit_predict(image)\n",
    "    palette = kmeans.cluster_centers_.astype(int)\n",
    "    \n",
    "    # ìƒ‰ìƒì˜ ì¶œí˜„ ë¹ˆë„ ê³„ì‚°\n",
    "    counter = np.bincount(labels)\n",
    "    total_count = np.sum(counter)\n",
    "    # ì£¼ìš” ìƒ‰ìƒ ë° ë¹„ìœ¨ ì €ì¥\n",
    "    color_ratios = {tuple(palette[i]): counter[i] / total_count for i in range(len(palette))}\n",
    "    # ìƒ‰ìƒëª…ì„ ê°€ì ¸ì˜¤ë„ë¡ ìˆ˜ì •\n",
    "    color_list = [(get_color_name_from_rgb(*color), ratio) for color, ratio in color_ratios.items()]\n",
    "    \n",
    "    return sorted(color_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ë°ê¸° ë° ëŒ€ë¹„ ê³„ì‚°\n",
    "def calculate_brightness(image):\n",
    "    return np.mean(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "def calculate_contrast(image):\n",
    "    return np.std(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "# ì¸ë„¤ì¼ ë¶„ì„ í•¨ìˆ˜\n",
    "def analyze_image(image_url):\n",
    "    image, height, width = load_image_from_url(image_url)\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    # Step 1: ì´ë¯¸ì§€ íŠ¹ì„± ì¶”ì¶œ (ìƒ‰ìƒ, ë°ê¸°, ëŒ€ë¹„)\n",
    "    colors = extract_colors(image)\n",
    "    brightness = calculate_brightness(image)\n",
    "    contrast = calculate_contrast(image)\n",
    "    \n",
    "    # Step 2: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í›„ OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    processed_image = preprocess_image(image)\n",
    "    text_data = extract_text(processed_image, confidence_threshold=0.7)\n",
    "    \n",
    "    # Step 3: ê°ì²´ íƒì§€ (YOLOv5)\n",
    "    object_data = detect_objects_with_yolov5(image)\n",
    "    \n",
    "    # ê²°ê³¼ ë¦¬í„´\n",
    "    result = {\n",
    "        \"dominant_colors\": colors,\n",
    "        \"brightness\": brightness,\n",
    "        \"contrast\": contrast,\n",
    "        \"text_details\": text_data,\n",
    "        \"object_details\": object_data\n",
    "    }\n",
    "    \n",
    "    return result, height, width\n",
    "\n",
    "# Get the size of the thumbnail image (width, height)\n",
    "def get_thumbnail_size(url):\n",
    "    image, height, width = load_image_from_url(url)\n",
    "    return (height, width) if image is not None else (None, None)\n",
    "\n",
    "# Add 'thumbnail_size' column\n",
    "df['thumbnail_size'] = df['thumbnail_url'].apply(get_thumbnail_size)\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "okt = Okt()\n",
    "\n",
    "# ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸\n",
    "stopwords = set([\n",
    "    'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ë„', 'ë¡œ', 'ê³¼', 'ì™€', 'í•œ', 'í•˜ë‹¤',\n",
    "    'ì—ì„œ', 'ì—ê²Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ë³´ë‹¤', 'ì²˜ëŸ¼', 'ë§Œ', 'ì—†ì´', 'ìˆ˜', 'ê²ƒ', 'ì¢€', 'ë”', 'ì´',\n",
    "    'ë˜', 'ë“±', 'ê·¸', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ê²ƒ', 'ê±°', 'ë•Œ', 'ê±´', 'ì¤‘', 'ë‚˜', 'ë„ˆ', 'ì €', 'ìš°ë¦¬',\n",
    "    'ëˆ„êµ¬', 'ë­', 'ì™œ', 'ì–´ë””', 'ì–´ë–»ê²Œ', 'ì˜ìƒ', 'ì±„ë„', 'ì˜¤ëŠ˜', 'ì´ì œ', 'ì •ë§', 'ì§„ì§œ',\n",
    "    'ì™„ì „', 'ê·¸ëƒ¥', 'ë‚´ê°€', 'ë‹¹ì‹ ', 'ë‚´ìš©', 'ì œëª©', 'ì‹œì‘', 'ë', 'ë‹¤ì‹œ', 'ëª¨ë‘',\n",
    "    'ìµœê³ ', 'ëŒ€ë°•', 'ì†Œë¦„', 'í—', 'ã…‹ã…‹', 'ã…ã…', 'ã… ã… ', 'ì™€ìš°','ìë§‰','ë¸Œì´','ë¡œê·¸','ì¼ìƒ',\n",
    "    'ì •ë³´', 'í•„ë…', 'ì—…ë¡œë“œ', 'ìë§‰', 'êµ¬ë…', 'ì¢‹ì•„ìš”', 'ëŒ“ê¸€', 'ì‹œì²­', 'í™•ì¸',\n",
    "    'ë³´ì„¸ìš”'\n",
    "])\n",
    "\n",
    "# í´ë¦­ ìœ ë„ í‚¤ì›Œë“œ\n",
    "clickbait_keywords = [\n",
    "    'ì‹¤í™”', 'ì¶©ê²©', 'ëŒ€ë°•', 'ì†Œë¦„', 'ë°˜ì „', 'ìµœì´ˆ', 'ë“œë””ì–´', 'í—', 'ì§„ì‹¤',\n",
    "    'ë¯¿ê¸°ì§€', 'ì´ê²Œ', 'ë¬´ì¡°ê±´', 'ì£½ê¸° ì „ì—', 'ê¼­ ë´ì•¼í• '\n",
    "]\n",
    "\n",
    "# í”¼ì²˜ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def extract_korean_title_features(title, video_id):\n",
    "    features = {}\n",
    "    features['video_id'] = video_id\n",
    "    features['title'] = title\n",
    "    features['title_length'] = len(title)\n",
    "    features['word_count'] = len(okt.morphs(title))\n",
    "\n",
    "    # ì´ëª¨ì§€ ê´€ë ¨ í”¼ì²˜\n",
    "    features['emoji_count'] = sum(1 for char in title if char in emoji.EMOJI_DATA)\n",
    "    features['has_emoji'] = int(features['emoji_count'] > 0)\n",
    "\n",
    "    # íŠ¹ìˆ˜ë¬¸ì ìˆ˜\n",
    "    special_chars = re.findall(r\"[!\\\"#$%&'()*+,\\-./:;<=>?@\\[\\]^_`{|}~]\", title)\n",
    "    features['special_char_count'] = len(special_chars)\n",
    "\n",
    "    # í´ë¦­ ìœ ë„ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€\n",
    "    features['is_clickbait'] = int(any(word in title for word in clickbait_keywords))\n",
    "\n",
    "    # êµ¬ë‘ì  í¬í•¨ ì—¬ë¶€\n",
    "    features['has_question_mark'] = '?' in title\n",
    "    features['has_exclamation'] = '!' in title\n",
    "\n",
    "    # ì£¼ìš” ëª…ì‚¬ 3ê°œ ì¶”ì¶œ (ë¶ˆìš©ì–´ ì œê±° í¬í•¨)\n",
    "    nouns = okt.nouns(title)\n",
    "    filtered_nouns = [noun for noun in nouns if noun not in stopwords and len(noun) > 1]\n",
    "    noun_freq = Counter(filtered_nouns)\n",
    "    top_nouns = [word for word, _ in noun_freq.most_common(3)]\n",
    "    for i in range(3):\n",
    "        features[f'top_noun_{i+1}'] = top_nouns[i] if i < len(top_nouns) else ''\n",
    "\n",
    "    return features\n",
    "\n",
    "# Extract features from title and thumbnail\n",
    "title_features = extract_korean_title_features(df.iloc[0, 0], '0')\n",
    "thumbnail_features, height, width = analyze_image(df.iloc[0, 1])\n",
    "\n",
    "# Add features to the DataFrame\n",
    "# Title features\n",
    "for key, value in title_features.items():\n",
    "    df[key] = value\n",
    "\n",
    "# Thumbnail features (dominant_colors, brightness, contrast, etc.)\n",
    "df['dominant_colors'] = [', '.join([color[0] for color in thumbnail_features['dominant_colors']])]\n",
    "\n",
    "df['brightness'] = thumbnail_features['brightness']\n",
    "df['contrast'] = thumbnail_features['contrast']\n",
    "df['text_details'] = [thumbnail_features['text_details']]  # Keep the structure intact for text details\n",
    "df['object_details'] = [thumbnail_features['object_details']]  # Keep the structure intact for object details\n",
    "\n",
    "def classify_positions(image_width, image_height, text_details, object_details):\n",
    "    def classify_position(x, y, width, height):\n",
    "        \"\"\" ìœ„ì¹˜ë¥¼ ì™¼ìª½/ì¤‘ê°„/ì˜¤ë¥¸ìª½, ìœ„/ì¤‘ê°„/ì•„ë˜, í¬ê¸°ë³„ë¡œ ë¶„ë¥˜ \"\"\"\n",
    "        right_x = x + width\n",
    "        bottom_y = y + height\n",
    "\n",
    "        # ê°€ë¡œ ìœ„ì¹˜ (left, middle, right)\n",
    "        if right_x < image_width * 0.33:\n",
    "            horizontal_position = \"left\"\n",
    "        elif x > image_width * 0.67:\n",
    "            horizontal_position = \"right\"\n",
    "        else:\n",
    "            horizontal_position = \"middle\"\n",
    "\n",
    "        # ì„¸ë¡œ ìœ„ì¹˜ (up, middle, down)\n",
    "        if bottom_y < image_height * 0.33:\n",
    "            vertical_position = \"up\"\n",
    "        elif y > image_height * 0.67:\n",
    "            vertical_position = \"down\"\n",
    "        else:\n",
    "            vertical_position = \"middle\"\n",
    "\n",
    "        # í¬ê¸° ë¶„ë¥˜ (s, m, l)\n",
    "        area = width * height\n",
    "        size_category = \"s\" if area < 10000 else (\"m\" if area < 30000 else \"l\")\n",
    "\n",
    "        return f\"{horizontal_position} {vertical_position} {size_category}\"\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ë¶„ì„\n",
    "    text_positions = [classify_position(td['x'], td['y'], td['width'], td['height']) for td in text_details]\n",
    "    if not text_positions:  # í…ìŠ¤íŠ¸ê°€ ì—†ì„ ê²½ìš°\n",
    "        text_positions = [\"í…ìŠ¤íŠ¸ ì—†ìŒ\"]\n",
    "\n",
    "    # ì‚¬ëŒ ìœ„ì¹˜ ë¶„ì„\n",
    "    person_positions = []\n",
    "    \n",
    "    # Ensure 'object_details' is a dictionary and contains 'objects' key\n",
    "    if isinstance(object_details, dict) and 'objects' in object_details:\n",
    "        for obj in object_details['objects']:\n",
    "            if isinstance(obj, dict) and obj.get('label') == 'person':\n",
    "                # 'person' ê°ì²´ë§Œ ìœ„ì¹˜ ë¶„ë¥˜\n",
    "                position = classify_position(obj['x'], obj['y'], obj['width'], obj['height'])\n",
    "                person_positions.append(position)\n",
    "\n",
    "    if not person_positions:  # ì‚¬ëŒ ê°ì²´ê°€ ì—†ì„ ê²½ìš°\n",
    "        person_positions = [\"ì‚¬ëŒ ì—†ìŒ\"]\n",
    "\n",
    "    # ì‹ ë¢°ë„ ë†’ì€ í…ìŠ¤íŠ¸ ì¶”ì¶œ (probability â‰¥ 0.7)\n",
    "    high_confidence_texts = [td[\"text\"] for td in text_details if td.get(\"probability\", 0) >= 0.7]\n",
    "    prob_text = high_confidence_texts if high_confidence_texts else [\"í•´ë‹¹ ì—†ìŒ\"]\n",
    "\n",
    "    return text_positions, person_positions, prob_text\n",
    "\n",
    "\n",
    "# Apply the function to classify text and person positions\n",
    "df[[\"text_positions\", \"person_positions\", \"prob_text\"]] = df.apply(\n",
    "    lambda row: pd.Series(\n",
    "        classify_positions(\n",
    "            image_width=row[\"thumbnail_size\"][1],   # width (ë‘ ë²ˆì§¸ ìš”ì†Œ)\n",
    "            image_height=row[\"thumbnail_size\"][0],  # height (ì²« ë²ˆì§¸ ìš”ì†Œ)\n",
    "            text_details=ast.literal_eval(row[\"text_details\"]) if isinstance(row[\"text_details\"], str) else row[\"text_details\"],\n",
    "            object_details=ast.literal_eval(row[\"object_details\"]) if isinstance(row[\"object_details\"], str) else row[\"object_details\"]\n",
    "        )\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Ensure that 'person_positions' is a list, if it's a string, convert it to list\n",
    "df['person_positions'] = df['person_positions'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "def extract_labels(obj_detail_str):\n",
    "    try:\n",
    "        # If obj_detail_str is a string, convert it to a dictionary\n",
    "        obj_details = ast.literal_eval(obj_detail_str) if isinstance(obj_detail_str, str) else obj_detail_str\n",
    "        \n",
    "        # Check if the 'objects' key exists and it contains a list\n",
    "        if isinstance(obj_details, dict) and 'objects' in obj_details and isinstance(obj_details['objects'], list):\n",
    "            # Extract labels from the objects list\n",
    "            labels = [obj[\"label\"] for obj in obj_details['objects'] if \"label\" in obj]\n",
    "            return labels if labels else [\"ì—†ìŒ\"]\n",
    "        else:\n",
    "            return [\"ì—ëŸ¬\"]\n",
    "    except Exception as e:\n",
    "        # Handle any errors (e.g., invalid format, missing 'objects' key)\n",
    "        print(f\"Error extracting labels: {e}\")\n",
    "        return [\"ì—ëŸ¬\"]\n",
    "\n",
    "df[\"object_labels\"] = df[\"object_details\"].apply(extract_labels)\n",
    "\n",
    "\n",
    "# ë¬¸ìì—´ì„ ì‹¤ì œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (object_labels ì¹¼ëŸ¼ì—ë§Œ ì ìš©)\n",
    "df['object_labels'] = df['object_labels'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "def safe_literal_eval(val):\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            return ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return []\n",
    "    return val if isinstance(val, list) else []\n",
    "\n",
    "df['person_count'] = df['object_labels'].apply(lambda x: x.count('person') if isinstance(x, list) else 0)\n",
    "df['object_count'] = df['object_labels'].apply(lambda x: len([obj for obj in x if obj != 'person']) if isinstance(x, list) else 0)\n",
    "\n",
    "df['has_text'] = df['text_positions'].apply(lambda x: int(x != ['í…ìŠ¤íŠ¸ ì—†ìŒ']))\n",
    "\n",
    "# ì‚¬ëŒ ìœ„ì¹˜\n",
    "df['person_left'] = df['person_positions'].apply(lambda x: sum('left' in p for p in x))\n",
    "df['person_middle'] = df['person_positions'].apply(lambda x: sum('middle' in p for p in x))\n",
    "df['person_right'] = df['person_positions'].apply(lambda x: sum('right' in p for p in x))\n",
    "df['person_small'] = df['person_positions'].apply(lambda x: sum('s' in p for p in x))\n",
    "df['person_medium'] = df['person_positions'].apply(lambda x: sum('m' in p for p in x))\n",
    "df['person_large'] = df['person_positions'].apply(lambda x: sum('l' in p for p in x))\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ìœ„ì¹˜\n",
    "df['text_left'] = df['text_positions'].apply(lambda x: sum('left' in p for p in x))\n",
    "df['text_middle'] = df['text_positions'].apply(lambda x: sum('middle' in p for p in x))\n",
    "df['text_right'] = df['text_positions'].apply(lambda x: sum('right' in p for p in x))\n",
    "df['text_small'] = df['text_positions'].apply(lambda x: sum('s' in p for p in x))\n",
    "df['text_medium'] = df['text_positions'].apply(lambda x: sum('m' in p for p in x))\n",
    "df['text_large'] = df['text_positions'].apply(lambda x: sum('l' in p for p in x))\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import webcolors\n",
    "\n",
    "# ì›¹ ìƒ‰ìƒ ì´ë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë” ê´‘ë²”ìœ„í•œ ìƒ‰ìƒ ê·¸ë£¹ ì„¤ì •\n",
    "color_groups = {\n",
    "    'red': {'red', 'crimson', 'firebrick', 'darkred', 'salmon', 'indianred', 'tomato', 'orangered', 'darkorange', 'lightcoral', 'maroon', 'brown'},\n",
    "    'blue': {'blue', 'navy', 'dodgerblue', 'deepskyblue', 'royalblue', 'skyblue', 'slateblue', 'mediumblue', 'cornflowerblue', 'steelblue', 'lightblue', 'powderblue', 'midnightblue', 'lightsteelblue'},\n",
    "    'green': {'green', 'lime', 'forestgreen', 'seagreen', 'springgreen', 'mediumseagreen', 'darkgreen', 'lawngreen', 'yellowgreen', 'olive', 'olivedrab', 'chartreuse'},\n",
    "    'yellow': {'yellow', 'gold', 'khaki', 'lemonchiffon', 'lightyellow', 'palegoldenrod', 'lightgoldenrodyellow', 'goldenrod', 'darkgoldenrod'},\n",
    "    'purple': {'purple', 'magenta', 'violet', 'orchid', 'mediumorchid', 'mediumpurple', 'darkviolet', 'blueviolet', 'darkorchid', 'thistle', 'lavender', 'plum'},\n",
    "    'brown': {'brown', 'sienna', 'chocolate', 'peru', 'saddlebrown', 'tan', 'burlywood', 'rosybrown', 'darkkhaki', 'khaki'},\n",
    "    'grey': {'grey', 'gray', 'lightgrey', 'darkgrey', 'dimgrey', 'slategrey', 'gainsboro', 'darkslategrey', 'lightsteelblue', 'silver', 'dimgray'},\n",
    "    'white': {'white', 'snow', 'ivory', 'ghostwhite', 'whitesmoke', 'floralwhite', 'seashell', 'beige', 'linen', 'mintcream', 'seashell'},\n",
    "    'pink': {'pink', 'lightpink', 'hotpink', 'lavenderblush', 'deeppink', 'mediumvioletred', 'palevioletred'},\n",
    "    'black': {'black', 'darkslategray', 'dimgray', 'charcoal'},\n",
    "    'other': set()\n",
    "}\n",
    "\n",
    "# ìƒ‰ìƒì„ ê·¸ë£¹ìœ¼ë¡œ ë§¤í•‘\n",
    "def map_color_to_group(color):\n",
    "    for group, colors in color_groups.items():\n",
    "        if color.lower() in colors:\n",
    "            return group\n",
    "    return 'other'\n",
    "\n",
    "# ê° ìƒ‰ìƒ ê·¸ë£¹ë³„ë¡œ 0.0 ì´ˆê¸°í™”\n",
    "for col in color_groups.keys():\n",
    "    df[f'color_{col}'] = 0.0\n",
    "\n",
    "# ìƒ‰ìƒ ê·¸ë£¹ í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "def count_color_groups(color_list):\n",
    "    counter = defaultdict(float)\n",
    "    for color in color_list:\n",
    "        group = map_color_to_group(color)\n",
    "        counter[group] += 1  # ê° ìƒ‰ìƒì€ 1ì”© ì¶”ê°€ë¨\n",
    "    return counter\n",
    "\n",
    "# dominant_colors ì»¬ëŸ¼ì´ ë¬¸ìì—´ë¡œ ë˜ì–´ìˆìœ¼ë¯€ë¡œ, ì´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "for idx, row in df.iterrows():\n",
    "    if isinstance(row['dominant_colors'], str):\n",
    "        color_list = row['dominant_colors'].split(', ')  # ',' ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "        counter = count_color_groups(color_list)\n",
    "        for group in counter:\n",
    "            df.at[idx, f'color_{group}'] = counter[group]\n",
    "\n",
    "# ìƒ‰ìƒ ì»¬ëŸ¼ì´ 0ì´ë©´ 0, 0ì´ ì•„ë‹ˆë©´ 1ë¡œ ë³€í™˜\n",
    "color_columns = [\n",
    "    'color_red', 'color_blue', 'color_green', 'color_yellow',\n",
    "    'color_purple', 'color_brown', 'color_grey', 'color_white',\n",
    "    'color_pink', 'color_black', 'color_other'\n",
    "]\n",
    "\n",
    "df[color_columns] = df[color_columns].applymap(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "feature_cols = [\n",
    "    'duration', 'subscriber_count', 'brightness', 'contrast',\n",
    "       'title_length', 'word_count', 'emoji_count', 'has_emoji',\n",
    "       'special_char_count', 'is_clickbait', 'has_question_mark',\n",
    "       'has_exclamation', 'pub_year', 'pub_month', 'pub_weekday', 'color_red',\n",
    "       'color_blue', 'color_green', 'color_yellow', 'color_purple',\n",
    "       'color_brown', 'color_grey', 'color_white', 'color_pink',\n",
    "       'person_count', 'object_count', 'has_text', 'person_left',\n",
    "       'person_middle', 'person_right', 'person_small', 'person_medium',\n",
    "       'person_large', 'text_left', 'text_middle', 'text_right', 'text_small',\n",
    "       'text_medium', 'text_large'\n",
    "]\n",
    "model_df=df[feature_cols]\n",
    "\n",
    "\n",
    "\n",
    "import joblib  # or use pickle if you prefer\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ (ì˜ˆ: RandomForest, XGBoost, LinearRegression ë“±)\n",
    "model = joblib.load('saved_models/model_cluster_0.pkl')\n",
    "# ì˜ˆì¸¡ì— í•„ìš”í•œ íŠ¹ì„±ë§Œ ì„ íƒ (ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•œ íŠ¹ì„±ê³¼ ë™ì¼í•´ì•¼ í•¨)\n",
    "X = model_df[feature_cols]  # íŠ¹ì„±ë“¤ì´ ë‹´ê¸´ DataFrame\n",
    "# ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ DataFrameì— ì¶”ê°€\n",
    "model_df['predicted_views'] = y_pred\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥\n",
    "print(model_df['predicted_views'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d288696b-5b7b-4eff-a9a5-2a879ba0b922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duwjd\\AppData\\Local\\Temp\\ipykernel_820\\4124645825.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_df['subscriber_count']=100000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    10.027542\n",
      "Name: predicted_views, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duwjd\\AppData\\Local\\Temp\\ipykernel_820\\4124645825.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_df['predicted_views'] = y_pred\n"
     ]
    }
   ],
   "source": [
    "import joblib  # or use pickle if you prefer\n",
    "model_df['subscriber_count']=100000000\n",
    "# ëª¨ë¸ ë¡œë“œ (ì˜ˆ: RandomForest, XGBoost, LinearRegression ë“±)\n",
    "model = joblib.load('saved_models/model_cluster_3.pkl')\n",
    "# ì˜ˆì¸¡ì— í•„ìš”í•œ íŠ¹ì„±ë§Œ ì„ íƒ (ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•œ íŠ¹ì„±ê³¼ ë™ì¼í•´ì•¼ í•¨)\n",
    "X = model_df[feature_cols]  # íŠ¹ì„±ë“¤ì´ ë‹´ê¸´ DataFrame\n",
    "# ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ DataFrameì— ì¶”ê°€\n",
    "model_df['predicted_views'] = y_pred\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥\n",
    "print(model_df['predicted_views'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c329b46d-3813-4a4a-bc6d-cfbd4543a04b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
