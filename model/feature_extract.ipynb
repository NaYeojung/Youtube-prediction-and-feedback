{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04140e3d-f268-4ac4-8f75-603bab2cf07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import isodate\n",
    "import requests\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import easyocr\n",
    "import torch\n",
    "import webcolors\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e77ae-bd67-46bf-b424-e615f33da7c8",
   "metadata": {},
   "source": [
    "## Ïç∏ÎÑ§Ïùº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a161d72-1f26-4035-84d7-28b36f999d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO Î™®Îç∏ Î°úÎìú (Ultralytics YOLOv5 ÏòàÏ†ú)\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "ocr_reader = easyocr.Reader(['en', 'ko'])  # ÏòÅÏñ¥ & ÌïúÍ∏Ä OCR ÏßÄÏõê\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú (URLÏóêÏÑú Îã§Ïö¥Î°úÎìú)\n",
    "def load_image_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        image = np.asarray(bytearray(response.content), dtype=np.uint8)\n",
    "        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "        if image is None:\n",
    "            raise ValueError(\"Ïù¥ÎØ∏ÏßÄÎ•º ÎîîÏΩîÎî©Ìï† Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "        return image\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Ïù¥ÎØ∏ÏßÄÎ•º Î∂àÎü¨Ïò§Îäî Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}\")\n",
    "        return None\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ (Í∑∏Î†àÏù¥Ïä§ÏºÄÏùº + Ïù¥ÏßÑÌôî)\n",
    "def preprocess_image(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    denoised_image = cv2.GaussianBlur(binary_image, (5, 5), 0)\n",
    "    return denoised_image\n",
    "\n",
    "# ÌÖçÏä§Ìä∏ Î∂ÑÏÑù (OCR)\n",
    "def extract_text(image, confidence_threshold=0.7):\n",
    "    results = ocr_reader.readtext(image)\n",
    "    text_info = []\n",
    "    for (bbox, text, prob) in results:\n",
    "        if prob >= confidence_threshold: \n",
    "            (top_left, _, bottom_right, _) = bbox\n",
    "            x, y = int(top_left[0]), int(top_left[1])\n",
    "            width = int(bottom_right[0] - top_left[0])\n",
    "            height = int(bottom_right[1] - top_left[1])\n",
    "            area = width * height\n",
    "            \n",
    "            text_info.append({\n",
    "                \"text\": text,\n",
    "                \"x\": x, \"y\": y,\n",
    "                \"width\": width, \"height\": height,\n",
    "                \"area\": area,\n",
    "                \"probability\": prob \n",
    "            })\n",
    "    return text_info\n",
    "\n",
    "# Í∞ùÏ≤¥ ÌÉêÏßÄ (YOLO)\n",
    "def detect_objects(image):\n",
    "    if image is None:\n",
    "        return {\"objects\": [], \"central_focus\": False}\n",
    "    \n",
    "    results = yolo_model(image)\n",
    "    objects = []\n",
    "    central_focus = False\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    for result in results.xyxy[0]:  # YOLOv5 Í≤∞Í≥º\n",
    "        x1, y1, x2, y2, conf, cls = map(int, result[:6])\n",
    "        label = yolo_model.names[cls]\n",
    "        \n",
    "        # Í∞ùÏ≤¥Ïùò Ï§ëÏïô Ïó¨Î∂Ä ÌôïÏù∏\n",
    "        obj_center_x = (x1 + x2) / 2\n",
    "        obj_center_y = (y1 + y2) / 2\n",
    "        if (width * 0.3) < obj_center_x < (width * 0.7) and (height * 0.3) < obj_center_y < (height * 0.7):\n",
    "            central_focus = True\n",
    "        \n",
    "        objects.append({\"label\": label, \"x\": x1, \"y\": y1, \"width\": x2-x1, \"height\": y2-y1})\n",
    "    \n",
    "    return {\"objects\": objects, \"central_focus\": central_focus}\n",
    "\n",
    "\n",
    "# Ï£ºÏöî ÏÉâÏÉÅ Ï∂îÏ∂ú Î∞è Ïù¥Î¶Ñ Î≥ÄÌôò\n",
    "def closest_colour(requested_colour):\n",
    "    \"\"\"Ï£ºÏñ¥ÏßÑ RGB Í∞íÍ≥º Í∞ÄÏû• Í∞ÄÍπåÏö¥ CSS3 ÏÉâÏÉÅÎ™ÖÏùÑ Ï∞æÏùå\"\"\"\n",
    "    min_colours = {}\n",
    "    for name in webcolors.names(\"css3\"):\n",
    "        r_c, g_c, b_c = webcolors.name_to_rgb(name)\n",
    "        rd = (r_c - requested_colour[0]) ** 2\n",
    "        gd = (g_c - requested_colour[1]) ** 2\n",
    "        bd = (b_c - requested_colour[2]) ** 2\n",
    "        min_colours[(rd + gd + bd)] = name\n",
    "    return min_colours[min(min_colours.keys())]\n",
    "\n",
    "def get_color_name_from_rgb(r, g, b):\n",
    "    \"\"\"Ï†ïÌôïÌïú ÏÉâÏÉÅÏù¥ ÏûàÏúºÎ©¥ Î∞òÌôò, ÏóÜÏúºÎ©¥ Í∞ÄÏû• Í∞ÄÍπåÏö¥ ÏÉâÏÉÅÎ™Ö Î∞òÌôò\"\"\"\n",
    "    try:\n",
    "        return webcolors.rgb_to_name((r, g, b), spec='css3')\n",
    "    except ValueError:\n",
    "        return closest_colour((r, g, b))\n",
    "\n",
    "def extract_colors(image, num_colors=5):\n",
    "    \"\"\"Ïù¥ÎØ∏ÏßÄÏóêÏÑú Ï£ºÏöî ÏÉâÏÉÅÏùÑ Ï∂îÏ∂úÌïòÍ≥†, Í∞Å ÏÉâÏÉÅÏùò ÎπÑÏú®ÏùÑ Î∞òÌôò\"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).reshape((-1, 3))\n",
    "    kmeans = KMeans(n_clusters=num_colors, n_init=\"auto\")\n",
    "    labels = kmeans.fit_predict(image)\n",
    "    palette = kmeans.cluster_centers_.astype(int)\n",
    "    \n",
    "    # ÏÉâÏÉÅÏùò Ï∂úÌòÑ ÎπàÎèÑ Í≥ÑÏÇ∞\n",
    "    counter = np.bincount(labels)\n",
    "    total_count = np.sum(counter)\n",
    "    # Ï£ºÏöî ÏÉâÏÉÅ Î∞è ÎπÑÏú® Ï†ÄÏû•\n",
    "    color_ratios = {tuple(palette[i]): counter[i] / total_count for i in range(len(palette))}\n",
    "    # ÏÉâÏÉÅÎ™ÖÏùÑ Í∞ÄÏ†∏Ïò§ÎèÑÎ°ù ÏàòÏ†ï\n",
    "    color_list = [(get_color_name_from_rgb(*color), ratio) for color, ratio in color_ratios.items()]\n",
    "    \n",
    "    return sorted(color_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# üîπ Î∞ùÍ∏∞ Î∞è ÎåÄÎπÑ Í≥ÑÏÇ∞\n",
    "def calculate_brightness(image):\n",
    "    return np.mean(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "def calculate_contrast(image):\n",
    "    return np.std(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "# üîπ Ï†ÑÏ≤¥ Ïç∏ÎÑ§Ïùº Î∂ÑÏÑù Ìï®Ïàò\n",
    "def analyze_image(image_url):\n",
    "    image = load_image_from_url(image_url)\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    # Step 1: Ïù¥ÎØ∏ÏßÄ ÌäπÏÑ± Ï∂îÏ∂ú (ÏÉâÏÉÅ, Î∞ùÍ∏∞, ÎåÄÎπÑ)\n",
    "    colors = extract_colors(image)\n",
    "    brightness = calculate_brightness(image)\n",
    "    contrast = calculate_contrast(image)\n",
    "    \n",
    "    # Step 2: Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ ÌõÑ OCR ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú\n",
    "    processed_image = preprocess_image(image)\n",
    "    text_data = extract_text(processed_image, confidence_threshold=0.7)\n",
    "    \n",
    "    # Step 3: Í∞ùÏ≤¥ ÌÉêÏßÄ (YOLO)\n",
    "    object_data = detect_objects(image)\n",
    "    \n",
    "    # Í≤∞Í≥º Î¶¨ÌÑ¥\n",
    "    result = {\n",
    "        \"dominant_colors\": colors,\n",
    "        \"brightness\": brightness,\n",
    "        \"contrast\": contrast,\n",
    "        \"text_details\": text_data,\n",
    "        \"object_details\": object_data\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "brightness_li=[]\n",
    "contrast_li=[]\n",
    "dominant_colors_li=[]\n",
    "text_details_li=[]\n",
    "largest_text_li=[]\n",
    "objects_details_li=[]\n",
    "\n",
    "for i in df['thumbnail_url']:\n",
    "    image_url = i\n",
    "    analysis_result = analyze_image(image_url)\n",
    "    \n",
    "    brightness_li.append(analysis_result['brightness'])\n",
    "    contrast_li.append(analysis_result['contrast'])\n",
    "    dominant_colors_li.append(analysis_result['dominant_colors'])\n",
    "    text_details_li.append(analysis_result['text_details'])\n",
    "    objects_details_li.append(analysis_result['object_details']['objects'])\n",
    "\n",
    "df['brightness']=brightness_li\n",
    "df['contrast']=contrast_li\n",
    "df['dominant_colors']=dominant_colors_li\n",
    "df['text_details']=text_details_li\n",
    "df['objects_details']=objects_details_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b91d6-446b-49e1-a428-a77358b82392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_positions(image_width, image_height, text_details, object_details):\n",
    "    def classify_position(x, y, width, height):\n",
    "        \"\"\" ÏúÑÏπòÎ•º ÏôºÏ™Ω/Ï§ëÍ∞Ñ/Ïò§Î•∏Ï™Ω, ÏúÑ/Ï§ëÍ∞Ñ/ÏïÑÎûò, ÌÅ¨Í∏∞Î≥ÑÎ°ú Î∂ÑÎ•ò \"\"\"\n",
    "        right_x = x + width\n",
    "        bottom_y = y + height\n",
    "\n",
    "        # Í∞ÄÎ°ú ÏúÑÏπò (left, middle, right)\n",
    "        if right_x < image_width * 0.33:\n",
    "            horizontal_position = \"left\"\n",
    "        elif x > image_width * 0.67:\n",
    "            horizontal_position = \"right\"\n",
    "        else:\n",
    "            horizontal_position = \"middle\"\n",
    "\n",
    "        # ÏÑ∏Î°ú ÏúÑÏπò (up, middle, down)\n",
    "        if bottom_y < image_height * 0.33:\n",
    "            vertical_position = \"up\"\n",
    "        elif y > image_height * 0.67:\n",
    "            vertical_position = \"down\"\n",
    "        else:\n",
    "            vertical_position = \"middle\"\n",
    "\n",
    "        # ÌÅ¨Í∏∞ Î∂ÑÎ•ò (s, m, l)\n",
    "        area = width * height\n",
    "        size_category = \"s\" if area < 10000 else (\"m\" if area < 30000 else \"l\")\n",
    "\n",
    "        return f\"{horizontal_position} {vertical_position} {size_category}\"\n",
    "\n",
    "    # Text ÏúÑÏπò Î∂ÑÏÑù\n",
    "    text_positions = [classify_position(td['x'], td['y'], td['width'], td['height']) for td in text_details]\n",
    "    if not text_positions:  # ÌÖçÏä§Ìä∏Í∞Ä ÏóÜÏùÑ Í≤ΩÏö∞\n",
    "        text_positions = [\"ÌÖçÏä§Ìä∏ ÏóÜÏùå\"]\n",
    "\n",
    "    # Person ÏúÑÏπò Î∂ÑÏÑù\n",
    "    person_positions = [\n",
    "        classify_position(obj['x'], obj['y'], obj['width'], obj['height'])\n",
    "        for obj in object_details if obj['label'] == 'person'\n",
    "    ]\n",
    "    if not person_positions:  # ÏÇ¨Îûå Í∞ùÏ≤¥Í∞Ä ÏóÜÏùÑ Í≤ΩÏö∞\n",
    "        person_positions = [\"ÏÇ¨Îûå ÏóÜÏùå\"]\n",
    "\n",
    "    # Ïã†Î¢∞ÎèÑ ÎÜíÏùÄ ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú (probability ‚â• 0.7)\n",
    "    high_confidence_texts = [td[\"text\"] for td in text_details if td.get(\"probability\", 0) >= 0.7]\n",
    "    prob_text = high_confidence_texts if high_confidence_texts else [\"Ìï¥Îãπ ÏóÜÏùå\"]\n",
    "\n",
    "    return text_positions, person_positions, prob_text\n",
    "\n",
    "\n",
    "df[[\"text_positions\", \"person_positions\", \"prob_text\"]] = df.apply(\n",
    "    lambda row: pd.Series(\n",
    "        classify_positions(\n",
    "            image_width=800,  # Ïù¥ÎØ∏ÏßÄ Í∞ÄÎ°ú ÌÅ¨Í∏∞\n",
    "            image_height=600,  # Ïù¥ÎØ∏ÏßÄ ÏÑ∏Î°ú ÌÅ¨Í∏∞\n",
    "            text_details=row[\"text_details\"],\n",
    "            object_details=row[\"objects_details\"]\n",
    "        )\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262df99d-0653-4ffd-9ea9-13b42d8df3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_li=[]\n",
    "for i in df['objects_details']:\n",
    "    object_li=[]\n",
    "    for j in i:\n",
    "        object_li.append(j['label'])\n",
    "    objects_li.append(object_li)\n",
    "\n",
    "assert len(df) == len(objects_li), \"Length of text_position_li does not match the number of rows in df\"\n",
    "df['contain_object'] = objects_li\n",
    "\n",
    "del df['text_details']\n",
    "del df['objects_details']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc98f7-433c-4d31-bb3b-ea14352e651c",
   "metadata": {},
   "source": [
    "## Ï†úÎ™©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa3c85e-e0a6-43cc-a915-0ebd4cfbd1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# KoNLPyÏùò Okt ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî\n",
    "okt = Okt()\n",
    "\n",
    "# Î∂àÏö©Ïñ¥ Î¶¨Ïä§Ìä∏ (ÏòÅÏñ¥ Î∂àÏö©Ïñ¥ Î∞è ÌïúÍ∏Ä Î∂àÏö©Ïñ¥)\n",
    "stopwords_korean = [\n",
    "    'Ïù¥', 'Í∑∏', 'Ï†Ä', 'Í∑∏Î¶¨Í≥†', 'ÌïòÏßÄÎßå', 'Í∑∏ÎûòÏÑú', 'ÎòêÎäî', 'Ïôú', 'Ïñ¥ÎñªÍ≤å', 'ÎÇò', 'ÎÑà', 'Ï†ÄÌù¨', 'Ïö∞Î¶¨', 'Í∑∏ÎÖÄ', 'Í∑∏Ïùò', \n",
    "    'Í∑∏Îì§', 'Í∞ôÏùÄ', 'ÎßéÏùÄ', 'Îã§', 'Ï¢Ä', 'Í∑∏Î†áÏßÄÎßå', 'Ïó¨Í∏∞', 'Í±∞Í∏∞', 'Ïù¥Í≤É', 'Í∑∏Í≤É', 'Ïù¥Ïïº', 'Ìï†', 'ÏßÄÍ∏à', 'ÏãúÍ∞Ñ', 'Í≤É', \n",
    "    'Ïàò', 'Í∞ôÏù¥', 'ÎêòÎã§', 'ÌïòÎã§', 'ÏûàÎã§', 'ÏóÜÎã§', 'ÏúÑÌï¥', 'ÏôúÎÉêÌïòÎ©¥', 'ÌïòÍ∏∞', 'ÍπåÏßÄ', 'Ï¢Ä', 'ÎÇòÏ§ëÏóê'\n",
    "]\n",
    "stopwords_english = [\n",
    "    'the', 'and', 'a', 'an', 'in', 'on', 'at', 'for', 'with', 'about', 'as', 'by', 'of', 'to', 'from', 'that', 'which', \n",
    "    'who', 'whom', 'this', 'it', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'has', 'have', 'had', 'having', 'do', \n",
    "    'does', 'did', 'doing', 'doing', 'themselves', 'yours', 'ours', 'its', 'their', 'theirs', 'what', 'how', 'why', 'where', \n",
    "    'when', 'i', 'you', 'he', 'she', 'we', 'they', 'all', 'any', 'one', 'some', 'each', 'every', 'no', 'not', 'nor', 'only', \n",
    "    'own', 'same', 'so', 'than', 'too', 'very', 'just', 'don‚Äôt', 'should', 'now', 'up', 'down', 'here', 'there', 'when', \n",
    "    'where', 'why'\n",
    "]\n",
    "# Í∞êÏÑ± Î∂ÑÏÑùÏùÑ ÏúÑÌïú SentimentIntensityAnalyzer Ï¥àÍ∏∞Ìôî\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò (ÌäπÏàòÎ¨∏Ïûê Ï†úÍ±∞ Î∞è ÏÜåÎ¨∏Ïûê Î≥ÄÌôò)\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # ÌäπÏàòÎ¨∏Ïûê Ï†úÍ±∞\n",
    "    return text.lower()\n",
    "\n",
    "# Ï†úÎ™© Í∏∏Ïù¥ Í≥ÑÏÇ∞\n",
    "def calculate_title_length(title):\n",
    "    return len(title)\n",
    "\n",
    "# Í∞êÏÑ± Î∂ÑÏÑù Ìï®Ïàò\n",
    "def sentiment_analysis(title):\n",
    "    sentiment_score = analyzer.polarity_scores(title)\n",
    "    sentiment = 'positive' if sentiment_score['compound'] > 0 else 'negative' if sentiment_score['compound'] < 0 else 'neutral'\n",
    "    return sentiment\n",
    "\n",
    "# Ïù¥Î™®Ìã∞ÏΩò Ìè¨Ìï® Ïó¨Î∂Ä ÌôïÏù∏\n",
    "def contains_emoji(title):\n",
    "    return any(emoji.is_emoji(char) for char in title)\n",
    "\n",
    "def count_emojis(title):\n",
    "    return sum(1 for char in title if emoji.is_emoji(char))\n",
    "\n",
    "# ÌäπÏàòÎ¨∏Ïûê Ìè¨Ìï® Ïó¨Î∂Ä ÌôïÏù∏\n",
    "def contains_special_characters(title):\n",
    "    return bool(re.search(r'[^\\w\\s]', title))\n",
    "\n",
    "# ÌïµÏã¨ ÌÇ§ÏõåÎìú Ï∂îÏ∂ú Ìï®Ïàò (KoNLPyÎ•º ÏÇ¨Ïö©Ìïú Î™ÖÏÇ¨ Ï∂îÏ∂ú)\n",
    "def extract_keywords_korean(title):\n",
    "    nouns = okt.nouns(title)  # Î™ÖÏÇ¨Îßå Ï∂îÏ∂ú\n",
    "    filtered_nouns = [word for word in nouns if word not in stopwords_korean and word not in stopwords_english]\n",
    "    \n",
    "    if not filtered_nouns:  # ÎßåÏïΩ ÌïÑÌÑ∞ÎßÅÎêú Î™ÖÏÇ¨Í∞Ä ÏóÜÎã§Î©¥ Îπà Î¶¨Ïä§Ìä∏Î•º Î∞òÌôò\n",
    "        return []\n",
    "    \n",
    "    # Îã®Ïñ¥ ÎπàÎèÑ Í≥ÑÏÇ∞\n",
    "    vectorizer = CountVectorizer(stop_words=None, ngram_range=(1, 1))  # 1-gramÎßå Ï∂îÏ∂ú\n",
    "    X = vectorizer.fit_transform([' '.join(filtered_nouns)])\n",
    "    word_freq = dict(zip(vectorizer.get_feature_names_out(), X.toarray().flatten()))\n",
    "    \n",
    "    # ÏÉÅÏúÑ 3Í∞ú ÌÇ§ÏõåÎìú Ï∂îÏ∂ú\n",
    "    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [word for word, freq in sorted_words[:3]]\n",
    "\n",
    "# Ï†ÑÏ≤¥ ÌîºÏ≥ê Ï∂îÏ∂ú Ìï®Ïàò\n",
    "def extract_features_from_titles(df):\n",
    "    features = []\n",
    "    for title in df['title']:  # DataFrameÏóêÏÑú Ï†úÎ™©ÏùÑ ÌïòÎÇòÏî© Ï≤òÎ¶¨\n",
    "        preprocessed_title = preprocess_text(title)\n",
    "        title_length = calculate_title_length(title)\n",
    "        sentiment = sentiment_analysis(title)\n",
    "        has_emoji = contains_emoji(title)\n",
    "        count_emoji = count_emojis(title)\n",
    "        has_special_characters = contains_special_characters(title)\n",
    "        keywords = extract_keywords_korean(preprocessed_title)\n",
    "        \n",
    "        # Ï∂îÏ∂úÎêú ÌîºÏ≥êÎì§\n",
    "        features.append({\n",
    "            'title': title,\n",
    "            'title_length': title_length,\n",
    "            'sentiment': sentiment,\n",
    "            'has_emoji': has_emoji,\n",
    "            'emoji_count': count_emoji, \n",
    "            'has_special_characters': has_special_characters,\n",
    "            'keywords': keywords\n",
    "        })\n",
    "    \n",
    "    # ÌîºÏ≥êÎì§ÏùÑ DataFrameÏúºÎ°ú Î≥ÄÌôò\n",
    "    feature_df = pd.DataFrame(features)\n",
    "    return feature_df\n",
    "\n",
    "# ÌîºÏ≥ê Ï∂îÏ∂ú\n",
    "feature_df = extract_features_from_titles(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
