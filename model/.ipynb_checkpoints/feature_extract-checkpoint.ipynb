{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04140e3d-f268-4ac4-8f75-603bab2cf07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duwjd\\anaconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import isodate\n",
    "import requests\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import easyocr\n",
    "import torch\n",
    "import webcolors\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "829b1b90-9e9a-4d28-a589-a474934ac1a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3324\n",
      "                                               title     video_id  \\\n",
      "0                                  ê²°êµ­ ê¸´íŒ”ë¬¸ì‹  ì œê±° í•˜ëŠ” ì¡°ë‘íŒ”  t-zwVkTOZA8   \n",
      "1  [vlog] ê³ ë“±í•™êµ êµì‚¬ ì§ì¥ì¸ ë¸Œì´ë¡œê·¸ | í•™ë¶€ëª¨ ì´íšŒ ê·¸ë¦¬ê³  ìƒë‹´ ê°€ë“í•œ í•œ ...  AMwSEsFVRGg   \n",
      "2                             ì˜¤ì‚¬ì¹´&êµí†  ë¸Œì´ë¡œê·¸ â€¢ ì‡¼í•‘ì—ë¯¸ë¤ì—¬ìë“¤  PocqBywf0FU   \n",
      "3  ë§ˆë¼ë¡œì œì—½ë–¡+ë‹­ê¼¬ì¹˜ ê¿€ì¡°í•©â€¢ë…¸í‹°ë“œ ë”¸ê¸°ì „ë©”ë‰´ğŸ“ë¨¹ê³  ê²°êµ­ ì»µë¼ë©´2ê°œë¡œ ë§ˆë¬´ë¦¬â€¢ì˜›ë‚ í†µë‹­...  6eNhJ4LYr1k   \n",
      "4  [ì°ë ˆë””ìœ—ë¯¸] ì´ì   ë§í•  ìˆ˜ ìˆë‹¤ğŸ™„ ì—­ëŒ€ê¸‰ ë˜¥ì°¨ ì° ë‚‹ì—¬ì˜¤ë‹ˆë¼.ã…£ì´ê²Œ ì‹¤í™”ë¼ê³ ? êµ¬...  G9tXGudpOCo   \n",
      "\n",
      "  published_date                                      thumbnail_url  \\\n",
      "0       20250328  https://i.ytimg.com/vi/t-zwVkTOZA8/maxresdefau...   \n",
      "1       20250328  https://i.ytimg.com/vi/AMwSEsFVRGg/maxresdefau...   \n",
      "2       20250224  https://i.ytimg.com/vi/PocqBywf0FU/maxresdefau...   \n",
      "3       20250329  https://i.ytimg.com/vi/6eNhJ4LYr1k/maxresdefau...   \n",
      "4       20250328  https://i.ytimg.com/vi/G9tXGudpOCo/maxresdefau...   \n",
      "\n",
      "   view_count  like_count  comment_count duration  \\\n",
      "0    244420.0         NaN          221.0     7:39   \n",
      "1     11267.0       265.0           54.0    18:37   \n",
      "2     12158.0       315.0          110.0    21:46   \n",
      "3     14333.0       441.0           71.0    29:32   \n",
      "4     58409.0         NaN          118.0    18:17   \n",
      "\n",
      "                        channel_id  subscriber_count  \n",
      "0               ('ì¡°ë‘íŒ”', '@ì¡°ë‘íŒ”ì´ë¼ê³ ')          412000.0  \n",
      "1  ('HappyHojin', '@Happppyhojin')           46500.0  \n",
      "2            ('ê¶Œì˜ˆì™•ì™•', '@ye_one_e')           15700.0  \n",
      "3       ('ì§¸ë§', '@buttermellowday')           71700.0  \n",
      "4         ('ëƒ”ì§€ nyanji', '@quya_a')          296000.0  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"video_datas.csv\", encoding=\"utf-8-sig\")\n",
    "#df=df.iloc[:3,:]\n",
    "print(len(df))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef22a070-2522-4060-a28e-b880b5d6b6db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title                 0\n",
      "video_id              0\n",
      "published_date        0\n",
      "thumbnail_url         0\n",
      "view_count            0\n",
      "like_count           68\n",
      "comment_count       174\n",
      "duration              0\n",
      "channel_id            0\n",
      "subscriber_count      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())\n",
    "# print(set(df[df['channel_id'].str.contains('none', case=False, na=False)]['channel_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "112cc4de-a590-4591-9ab0-4384d7f794dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‚¨ì€ ë°ì´í„° ê°œìˆ˜: 2540\n"
     ]
    }
   ],
   "source": [
    "# ì¡°íšŒìˆ˜ ë‚®ì€ ë°ì´í„° ì œê±°\n",
    "lower_threshold = df[\"view_count\"].quantile(0.05)  # í•˜ìœ„ 5%\n",
    "df = df[df[\"view_count\"] > lower_threshold]\n",
    "\n",
    "# êµ¬ë…ì ìˆ˜ ì ì€ ì±„ë„ ì‚­ì œ\n",
    "df = df[df[\"subscriber_count\"] > 500]\n",
    "\n",
    "# 5ë…„ ì´ìƒ ëœ ì˜ìƒ ì œê±° (í˜„ì¬ ë‚ ì§œ ê¸°ì¤€)\n",
    "df[\"published_date\"] = pd.to_datetime(df[\"published_date\"])  # ë‚ ì§œ ë³€í™˜\n",
    "three_years_ago = pd.Timestamp.today() - pd.DateOffset(years=5)\n",
    "df = df[df[\"published_date\"] > three_years_ago]\n",
    "\n",
    "# ì—…ë¡œë“œëœ ì§€ í•˜ë£¨ë°–ì— ì•ˆ ëœ ì˜ìƒ ì‚­ì œ\n",
    "one_day_ago = pd.Timestamp.today() - pd.DateOffset(days=3)\n",
    "df = df[df[\"published_date\"] < one_day_ago]\n",
    "\n",
    "# ì¤‘ë³µ ë°ì´í„° ì œê±°\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# ê²°ì¸¡ê°’(NaN) ì œê±°\n",
    "# df = df.dropna()\n",
    "\n",
    "# ì˜ìƒ ê¸¸ì´ê°€ 1ë¶„~60ë¶„ì´ ì•„ë‹Œ ë°ì´í„° ì‚­ì œ (ì´ˆ ë‹¨ìœ„ ê¸°ì¤€)\n",
    "import re\n",
    "\n",
    "def convert_to_seconds(time_str):\n",
    "    parts = list(map(int, re.findall(r'\\d+', time_str)))  # ìˆ«ìë§Œ ì¶”ì¶œ\n",
    "    \n",
    "    if len(parts) == 3:  # HH:MM:SS í˜•ì‹\n",
    "        h, m, s = parts\n",
    "    elif len(parts) == 2:  # MM:SS í˜•ì‹\n",
    "        h, m, s = 0, parts[0], parts[1]\n",
    "    else:\n",
    "        return np.nan  # ì˜ëª»ëœ í˜•ì‹ ì²˜ë¦¬\n",
    "\n",
    "    return h * 3600 + m * 60 + s  # ì´ˆ ë‹¨ìœ„ ë³€í™˜\n",
    "\n",
    "# ë³€í™˜ ì ìš©\n",
    "df[\"duration\"] = df[\"duration\"].astype(str).apply(convert_to_seconds)\n",
    "\n",
    "# ë³€í™˜ í›„, ì˜ìƒ ê¸¸ì´ê°€ 1ë¶„~1ì‹œê°„ ë²”ìœ„ ë‚´ì— ìˆëŠ” ë°ì´í„°ë§Œ ìœ ì§€\n",
    "df = df[(df[\"duration\"] >= 60) & (df[\"duration\"] <= 3600)]\n",
    "\n",
    "# ì´ìƒì¹˜(ë„ˆë¬´ ë†’ì€ ì¡°íšŒìˆ˜, ë„ˆë¬´ ê¸´ ì˜ìƒ) ì œê±°\n",
    "# ì¡°íšŒìˆ˜ ì´ìƒì¹˜ (ìƒìœ„ 1% ì´ìƒ ì œê±°)\n",
    "upper_threshold_views = df[\"view_count\"].quantile(0.99)\n",
    "df = df[df[\"view_count\"] < upper_threshold_views]\n",
    "\n",
    "# ì˜ìƒ ê¸¸ì´ ì´ìƒì¹˜ (ìƒìœ„ 1% ì´ìƒ ì œê±°)\n",
    "upper_threshold_length = df[\"duration\"].quantile(0.99)\n",
    "df = df[df[\"duration\"] < upper_threshold_length]\n",
    "\n",
    "print(\"ë‚¨ì€ ë°ì´í„° ê°œìˆ˜:\", len(df))\n",
    "# df.to_csv(\"cleaned_youtube_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e77ae-bd67-46bf-b424-e615f33da7c8",
   "metadata": {},
   "source": [
    "## ì¸ë„¤ì¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a161d72-1f26-4035-84d7-28b36f999d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\duwjd/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-3-18 Python-3.11.7 torch-2.6.0+cu118 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "C:\\Users\\duwjd/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\duwjd/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\duwjd/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    }
   ],
   "source": [
    "# YOLO ëª¨ë¸ ë¡œë“œ (Ultralytics YOLOv5 ì˜ˆì œ)\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "ocr_reader = easyocr.Reader(['en', 'ko'])  # ì˜ì–´ & í•œê¸€ OCR ì§€ì›\n",
    "\n",
    "# ì´ë¯¸ì§€ ë¡œë“œ (URLì—ì„œ ë‹¤ìš´ë¡œë“œ)\n",
    "def load_image_from_url(url):\n",
    "    fallback_urls = [\n",
    "        url,\n",
    "        url.replace('maxresdefault', 'sddefault'),\n",
    "        url.replace('sddefault', 'hqdefault'),\n",
    "        url.replace('hqdefault', 'mqdefault'),\n",
    "        url.replace('mqdefault', 'default')\n",
    "    ]\n",
    "\n",
    "    for new_url in fallback_urls:\n",
    "        try:\n",
    "            response = requests.get(new_url, stream=True, timeout=5)\n",
    "            response.raise_for_status()\n",
    "            image = np.asarray(bytearray(response.content), dtype=np.uint8)\n",
    "            image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "            if image is not None:\n",
    "                height, width, _ = image.shape\n",
    "                print(f\"ì´ë¯¸ì§€ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {new_url}\")\n",
    "                return image, height, width\n",
    "        except requests.RequestException:\n",
    "            print(f\"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {new_url}\")\n",
    "\n",
    "    raise ValueError(\"ëª¨ë“  URLì—ì„œ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ê·¸ë ˆì´ìŠ¤ì¼€ì¼ + ì´ì§„í™”)\n",
    "def preprocess_image(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    denoised_image = cv2.GaussianBlur(binary_image, (5, 5), 0)\n",
    "    return denoised_image\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„ì„ (OCR)\n",
    "def extract_text(image, confidence_threshold=0.7):\n",
    "    results = ocr_reader.readtext(image)\n",
    "    text_info = []\n",
    "    for (bbox, text, prob) in results:\n",
    "        if prob >= confidence_threshold: \n",
    "            (top_left, _, bottom_right, _) = bbox\n",
    "            x, y = int(top_left[0]), int(top_left[1])\n",
    "            width = int(bottom_right[0] - top_left[0])\n",
    "            height = int(bottom_right[1] - top_left[1])\n",
    "            area = width * height\n",
    "            \n",
    "            text_info.append({\n",
    "                \"text\": text,\n",
    "                \"x\": x, \"y\": y,\n",
    "                \"width\": width, \"height\": height,\n",
    "                \"area\": area,\n",
    "                \"probability\": prob \n",
    "            })\n",
    "    return text_info\n",
    "\n",
    "# ê°ì²´ íƒì§€ (YOLO)\n",
    "def detect_objects(image):\n",
    "    if image is None:\n",
    "        return {\"objects\": [], \"central_focus\": False}\n",
    "    \n",
    "    results = yolo_model(image)\n",
    "    objects = []\n",
    "    central_focus = False\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    for result in results.xyxy[0]:  # YOLOv5 ê²°ê³¼\n",
    "        x1, y1, x2, y2, conf, cls = map(int, result[:6])\n",
    "        label = yolo_model.names[cls]\n",
    "        \n",
    "        # ê°ì²´ì˜ ì¤‘ì•™ ì—¬ë¶€ í™•ì¸\n",
    "        obj_center_x = (x1 + x2) / 2\n",
    "        obj_center_y = (y1 + y2) / 2\n",
    "        if (width * 0.3) < obj_center_x < (width * 0.7) and (height * 0.3) < obj_center_y < (height * 0.7):\n",
    "            central_focus = True\n",
    "        \n",
    "        objects.append({\"label\": label, \"x\": x1, \"y\": y1, \"width\": x2-x1, \"height\": y2-y1})\n",
    "    \n",
    "    return {\"objects\": objects, \"central_focus\": central_focus}\n",
    "\n",
    "\n",
    "# ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ ë° ì´ë¦„ ë³€í™˜\n",
    "def closest_colour(requested_colour):\n",
    "    \"\"\"ì£¼ì–´ì§„ RGB ê°’ê³¼ ê°€ì¥ ê°€ê¹Œìš´ CSS3 ìƒ‰ìƒëª…ì„ ì°¾ìŒ\"\"\"\n",
    "    min_colours = {}\n",
    "    for name in webcolors.names(\"css3\"):\n",
    "        r_c, g_c, b_c = webcolors.name_to_rgb(name)\n",
    "        rd = (r_c - requested_colour[0]) ** 2\n",
    "        gd = (g_c - requested_colour[1]) ** 2\n",
    "        bd = (b_c - requested_colour[2]) ** 2\n",
    "        min_colours[(rd + gd + bd)] = name\n",
    "    return min_colours[min(min_colours.keys())]\n",
    "\n",
    "def get_color_name_from_rgb(r, g, b):\n",
    "    \"\"\"ì •í™•í•œ ìƒ‰ìƒì´ ìˆìœ¼ë©´ ë°˜í™˜, ì—†ìœ¼ë©´ ê°€ì¥ ê°€ê¹Œìš´ ìƒ‰ìƒëª… ë°˜í™˜\"\"\"\n",
    "    try:\n",
    "        return webcolors.rgb_to_name((r, g, b), spec='css3')\n",
    "    except ValueError:\n",
    "        return closest_colour((r, g, b))\n",
    "\n",
    "def extract_colors(image, num_colors=5):\n",
    "    \"\"\"ì´ë¯¸ì§€ì—ì„œ ì£¼ìš” ìƒ‰ìƒì„ ì¶”ì¶œí•˜ê³ , ê° ìƒ‰ìƒì˜ ë¹„ìœ¨ì„ ë°˜í™˜\"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).reshape((-1, 3))\n",
    "    kmeans = KMeans(n_clusters=num_colors, n_init=\"auto\")\n",
    "    labels = kmeans.fit_predict(image)\n",
    "    palette = kmeans.cluster_centers_.astype(int)\n",
    "    \n",
    "    # ìƒ‰ìƒì˜ ì¶œí˜„ ë¹ˆë„ ê³„ì‚°\n",
    "    counter = np.bincount(labels)\n",
    "    total_count = np.sum(counter)\n",
    "    # ì£¼ìš” ìƒ‰ìƒ ë° ë¹„ìœ¨ ì €ì¥\n",
    "    color_ratios = {tuple(palette[i]): counter[i] / total_count for i in range(len(palette))}\n",
    "    # ìƒ‰ìƒëª…ì„ ê°€ì ¸ì˜¤ë„ë¡ ìˆ˜ì •\n",
    "    color_list = [(get_color_name_from_rgb(*color), ratio) for color, ratio in color_ratios.items()]\n",
    "    \n",
    "    return sorted(color_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ğŸ”¹ ë°ê¸° ë° ëŒ€ë¹„ ê³„ì‚°\n",
    "def calculate_brightness(image):\n",
    "    return np.mean(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "def calculate_contrast(image):\n",
    "    return np.std(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "# ğŸ”¹ ì „ì²´ ì¸ë„¤ì¼ ë¶„ì„ í•¨ìˆ˜\n",
    "def analyze_image(image_url):\n",
    "    image, height, width = load_image_from_url(image_url)\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    # Step 1: ì´ë¯¸ì§€ íŠ¹ì„± ì¶”ì¶œ (ìƒ‰ìƒ, ë°ê¸°, ëŒ€ë¹„)\n",
    "    colors = extract_colors(image)\n",
    "    brightness = calculate_brightness(image)\n",
    "    contrast = calculate_contrast(image)\n",
    "    \n",
    "    # Step 2: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í›„ OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    processed_image = preprocess_image(image)\n",
    "    text_data = extract_text(processed_image, confidence_threshold=0.7)\n",
    "    \n",
    "    # Step 3: ê°ì²´ íƒì§€ (YOLO)\n",
    "    object_data = detect_objects(image)\n",
    "    \n",
    "    # ê²°ê³¼ ë¦¬í„´\n",
    "    result = {\n",
    "        \"dominant_colors\": colors,\n",
    "        \"brightness\": brightness,\n",
    "        \"contrast\": contrast,\n",
    "        \"text_details\": text_data,\n",
    "        \"object_details\": object_data\n",
    "    }\n",
    "    \n",
    "    return result, height, width\n",
    "\n",
    "brightness_li=[]\n",
    "contrast_li=[]\n",
    "dominant_colors_li=[]\n",
    "text_details_li=[]\n",
    "largest_text_li=[]\n",
    "objects_details_li=[]\n",
    "thumbnail_size=[]\n",
    "\n",
    "for i in df['thumbnail_url']:\n",
    "    image_url = i\n",
    "    analysis_result, height, width = analyze_image(image_url)\n",
    "    thumbnail_size.append((height, width))\n",
    "    \n",
    "    brightness_li.append(analysis_result['brightness'])\n",
    "    contrast_li.append(analysis_result['contrast'])\n",
    "    dominant_colors_li.append(analysis_result['dominant_colors'])\n",
    "    text_details_li.append(analysis_result['text_details'])\n",
    "    objects_details_li.append(analysis_result['object_details']['objects'])\n",
    "\n",
    "df['brightness']=brightness_li\n",
    "df['contrast']=contrast_li\n",
    "df['dominant_colors']=dominant_colors_li\n",
    "df['text_details']=text_details_li\n",
    "df['objects_details']=objects_details_li\n",
    "df['thumbnail_size']=thumbnail_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa8b91d6-446b-49e1-a428-a77358b82392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_positions(image_width, image_height, text_details, object_details):\n",
    "    def classify_position(x, y, width, height):\n",
    "        \"\"\" ìœ„ì¹˜ë¥¼ ì™¼ìª½/ì¤‘ê°„/ì˜¤ë¥¸ìª½, ìœ„/ì¤‘ê°„/ì•„ë˜, í¬ê¸°ë³„ë¡œ ë¶„ë¥˜ \"\"\"\n",
    "        right_x = x + width\n",
    "        bottom_y = y + height\n",
    "\n",
    "        # ê°€ë¡œ ìœ„ì¹˜ (left, middle, right)\n",
    "        if right_x < image_width * 0.33:\n",
    "            horizontal_position = \"left\"\n",
    "        elif x > image_width * 0.67:\n",
    "            horizontal_position = \"right\"\n",
    "        else:\n",
    "            horizontal_position = \"middle\"\n",
    "\n",
    "        # ì„¸ë¡œ ìœ„ì¹˜ (up, middle, down)\n",
    "        if bottom_y < image_height * 0.33:\n",
    "            vertical_position = \"up\"\n",
    "        elif y > image_height * 0.67:\n",
    "            vertical_position = \"down\"\n",
    "        else:\n",
    "            vertical_position = \"middle\"\n",
    "\n",
    "        # í¬ê¸° ë¶„ë¥˜ (s, m, l)\n",
    "        area = width * height\n",
    "        size_category = \"s\" if area < 10000 else (\"m\" if area < 30000 else \"l\")\n",
    "\n",
    "        return f\"{horizontal_position} {vertical_position} {size_category}\"\n",
    "\n",
    "    # Text ìœ„ì¹˜ ë¶„ì„\n",
    "    text_positions = [classify_position(td['x'], td['y'], td['width'], td['height']) for td in text_details]\n",
    "    if not text_positions:  # í…ìŠ¤íŠ¸ê°€ ì—†ì„ ê²½ìš°\n",
    "        text_positions = [\"í…ìŠ¤íŠ¸ ì—†ìŒ\"]\n",
    "\n",
    "    # Person ìœ„ì¹˜ ë¶„ì„\n",
    "    person_positions = [\n",
    "        classify_position(obj['x'], obj['y'], obj['width'], obj['height'])\n",
    "        for obj in object_details if obj['label'] == 'person'\n",
    "    ]\n",
    "    if not person_positions:  # ì‚¬ëŒ ê°ì²´ê°€ ì—†ì„ ê²½ìš°\n",
    "        person_positions = [\"ì‚¬ëŒ ì—†ìŒ\"]\n",
    "\n",
    "    # ì‹ ë¢°ë„ ë†’ì€ í…ìŠ¤íŠ¸ ì¶”ì¶œ (probability â‰¥ 0.7)\n",
    "    high_confidence_texts = [td[\"text\"] for td in text_details if td.get(\"probability\", 0) >= 0.7]\n",
    "    prob_text = high_confidence_texts if high_confidence_texts else [\"í•´ë‹¹ ì—†ìŒ\"]\n",
    "\n",
    "    return text_positions, person_positions, prob_text\n",
    "\n",
    "\n",
    "df[[\"text_positions\", \"person_positions\", \"prob_text\"]] = df.apply(\n",
    "    lambda row: pd.Series(\n",
    "        classify_positions(\n",
    "            image_width=width,  # ì´ë¯¸ì§€ ê°€ë¡œ í¬ê¸°\n",
    "            image_height=height,  # ì´ë¯¸ì§€ ì„¸ë¡œ í¬ê¸°\n",
    "            text_details=row[\"text_details\"],\n",
    "            object_details=row[\"objects_details\"]\n",
    "        )\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "262df99d-0653-4ffd-9ea9-13b42d8df3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_li=[]\n",
    "for i in df['objects_details']:\n",
    "    object_li=[]\n",
    "    for j in i:\n",
    "        object_li.append(j['label'])\n",
    "    objects_li.append(object_li)\n",
    "\n",
    "assert len(df) == len(objects_li), \"Length of text_position_li does not match the number of rows in df\"\n",
    "df['contain_object'] = objects_li\n",
    "\n",
    "del df['text_details']\n",
    "del df['objects_details']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc98f7-433c-4d31-bb3b-ea14352e651c",
   "metadata": {},
   "source": [
    "## ì œëª©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa3c85e-e0a6-43cc-a915-0ebd4cfbd1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# KoNLPyì˜ Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "okt = Okt()\n",
    "\n",
    "# ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ì˜ì–´ ë¶ˆìš©ì–´ ë° í•œê¸€ ë¶ˆìš©ì–´)\n",
    "stopwords_korean = [\n",
    "    'ì´', 'ê·¸', 'ì €', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ë˜ì„œ', 'ë˜ëŠ”', 'ì™œ', 'ì–´ë–»ê²Œ', 'ë‚˜', 'ë„ˆ', 'ì €í¬', 'ìš°ë¦¬', 'ê·¸ë…€', 'ê·¸ì˜', \n",
    "    'ê·¸ë“¤', 'ê°™ì€', 'ë§ì€', 'ë‹¤', 'ì¢€', 'ê·¸ë ‡ì§€ë§Œ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì´ì•¼', 'í• ', 'ì§€ê¸ˆ', 'ì‹œê°„', 'ê²ƒ', \n",
    "    'ìˆ˜', 'ê°™ì´', 'ë˜ë‹¤', 'í•˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ìœ„í•´', 'ì™œëƒí•˜ë©´', 'í•˜ê¸°', 'ê¹Œì§€', 'ì¢€', 'ë‚˜ì¤‘ì—'\n",
    "]\n",
    "stopwords_english = [\n",
    "    'the', 'and', 'a', 'an', 'in', 'on', 'at', 'for', 'with', 'about', 'as', 'by', 'of', 'to', 'from', 'that', 'which', \n",
    "    'who', 'whom', 'this', 'it', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'has', 'have', 'had', 'having', 'do', \n",
    "    'does', 'did', 'doing', 'doing', 'themselves', 'yours', 'ours', 'its', 'their', 'theirs', 'what', 'how', 'why', 'where', \n",
    "    'when', 'i', 'you', 'he', 'she', 'we', 'they', 'all', 'any', 'one', 'some', 'each', 'every', 'no', 'not', 'nor', 'only', \n",
    "    'own', 'same', 'so', 'than', 'too', 'very', 'just', 'donâ€™t', 'should', 'now', 'up', 'down', 'here', 'there', 'when', \n",
    "    'where', 'why'\n",
    "]\n",
    "# ê°ì„± ë¶„ì„ì„ ìœ„í•œ SentimentIntensityAnalyzer ì´ˆê¸°í™”\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ (íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜)\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    return text.lower()\n",
    "\n",
    "# ì œëª© ê¸¸ì´ ê³„ì‚°\n",
    "def calculate_title_length(title):\n",
    "    return len(title)\n",
    "\n",
    "# ê°ì„± ë¶„ì„ í•¨ìˆ˜\n",
    "def sentiment_analysis(title):\n",
    "    sentiment_score = analyzer.polarity_scores(title)\n",
    "    sentiment = 'positive' if sentiment_score['compound'] > 0 else 'negative' if sentiment_score['compound'] < 0 else 'neutral'\n",
    "    return sentiment\n",
    "\n",
    "# ì´ëª¨í‹°ì½˜ í¬í•¨ ì—¬ë¶€ í™•ì¸\n",
    "def contains_emoji(title):\n",
    "    return any(emoji.is_emoji(char) for char in title)\n",
    "\n",
    "def count_emojis(title):\n",
    "    return sum(1 for char in title if emoji.is_emoji(char))\n",
    "\n",
    "# íŠ¹ìˆ˜ë¬¸ì í¬í•¨ ì—¬ë¶€ í™•ì¸\n",
    "def contains_special_characters(title):\n",
    "    return bool(re.search(r'[^\\w\\s]', title))\n",
    "\n",
    "# í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ (KoNLPyë¥¼ ì‚¬ìš©í•œ ëª…ì‚¬ ì¶”ì¶œ)\n",
    "def extract_keywords_korean(title):\n",
    "    nouns = okt.nouns(title)  # ëª…ì‚¬ë§Œ ì¶”ì¶œ\n",
    "    filtered_nouns = [word for word in nouns if word not in stopwords_korean and word not in stopwords_english]\n",
    "    \n",
    "    if not filtered_nouns:  # ë§Œì•½ í•„í„°ë§ëœ ëª…ì‚¬ê°€ ì—†ë‹¤ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜\n",
    "        return []\n",
    "    \n",
    "    # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°\n",
    "    vectorizer = CountVectorizer(stop_words=None, ngram_range=(1, 1))  # 1-gramë§Œ ì¶”ì¶œ\n",
    "    X = vectorizer.fit_transform([' '.join(filtered_nouns)])\n",
    "    word_freq = dict(zip(vectorizer.get_feature_names_out(), X.toarray().flatten()))\n",
    "    \n",
    "    # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [word for word, freq in sorted_words[:3]]\n",
    "\n",
    "# ì „ì²´ í”¼ì³ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def extract_features_from_titles(df):\n",
    "    features = []\n",
    "    for title in df['title']:  # DataFrameì—ì„œ ì œëª©ì„ í•˜ë‚˜ì”© ì²˜ë¦¬\n",
    "        preprocessed_title = preprocess_text(title)\n",
    "        title_length = calculate_title_length(title)\n",
    "        sentiment = sentiment_analysis(title)\n",
    "        has_emoji = contains_emoji(title)\n",
    "        count_emoji = count_emojis(title)\n",
    "        has_special_characters = contains_special_characters(title)\n",
    "        keywords = extract_keywords_korean(preprocessed_title)\n",
    "        \n",
    "        # ì¶”ì¶œëœ í”¼ì³ë“¤\n",
    "        features.append({\n",
    "            'title': title,\n",
    "            'title_length': title_length,\n",
    "            'sentiment': sentiment,\n",
    "            'has_emoji': has_emoji,\n",
    "            'emoji_count': count_emoji, \n",
    "            'has_special_characters': has_special_characters,\n",
    "            'keywords': keywords\n",
    "        })\n",
    "    \n",
    "    # í”¼ì³ë“¤ì„ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    feature_df = pd.DataFrame(features)\n",
    "    return feature_df\n",
    "\n",
    "# í”¼ì³ ì¶”ì¶œ\n",
    "feature_df = extract_features_from_titles(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
